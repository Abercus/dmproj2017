{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for finding duplicate data\n",
    "\n",
    "TODO:\n",
    "\n",
    "1) For each broker separately - look how much decrease...\n",
    "\n",
    "2) Phase 2...\n",
    "\n",
    "Following notebook is to find out how much duplicate requests.\n",
    "\n",
    "\n",
    "With each request_uuid we have parameters - pickup_timetamp, return_timestamp, broker_contract, driver_age the same..\n",
    "\n",
    "Duplicates can be spotted as following:\n",
    "\n",
    "1) Same request_id\n",
    "\n",
    "2) Different request_id, but close time, same broker and same other parameters (age, source...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (14, 6)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2997586, 10)\n"
     ]
    }
   ],
   "source": [
    "# Read data..\n",
    "\n",
    "fields = [\"timestamp\", \"pickup_timestamp\", \"return_timestamp\", \"broker_contract_id\", \"driver_age\", \n",
    "          \"request_uuid\", \"source_country_region_id\", \"pickup_desk_id\", \"return_desk_id\", \"rental_days\"]\n",
    "df = pd.read_csv(\"rate_quote.csv\", skipinitialspace=True, usecols=fields)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 3)\n",
      "Contract_ID       int64\n",
      "Contract_name    object\n",
      "Broker name      object\n",
      "dtype: object\n",
      "timestamp                   datetime64[ns]\n",
      "pickup_desk_id                       int64\n",
      "return_desk_id                       int64\n",
      "pickup_timestamp            datetime64[ns]\n",
      "return_timestamp            datetime64[ns]\n",
      "rental_days                          int64\n",
      "broker_contract_id                   int64\n",
      "source_country_region_id             int64\n",
      "driver_age                           int64\n",
      "request_uuid                        object\n",
      "Contract_name                       object\n",
      "Broker_name                         object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df[\"timestamp\"] = pd.to_datetime(df['timestamp'])\n",
    "df[\"pickup_timestamp\"] = pd.to_datetime(df['pickup_timestamp'])\n",
    "df[\"return_timestamp\"] = pd.to_datetime(df['return_timestamp'])\n",
    "\n",
    "\n",
    "# Merge with broker contract name\n",
    "fields = [\"Contract_ID\", \"Contract_name\", \"Broker name\"]\n",
    "contract_df = pd.read_csv(\"broker_contracts.csv\", skipinitialspace=True, usecols=fields)\n",
    "# This holds contract names and which broker name corresponds to each contract id..\n",
    "#contract_df = contract_df.set_index(\"Contract_ID\")\n",
    "\n",
    "print(contract_df.shape)\n",
    "print(contract_df.dtypes)\n",
    "\n",
    "\n",
    "contract_df.columns = [\"broker_contract_id\", \"Contract_name\", \"Broker_name\"]\n",
    "    \n",
    "#print(df)\n",
    "    \n",
    "    \n",
    "merged_requests = pd.merge(df, contract_df, on=[\"broker_contract_id\"])\n",
    "\n",
    "print(merged_requests.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 - Removing duplicates using request_uuid\n",
    "\n",
    "1) Load in all data\n",
    "\n",
    "2) Group data by request_uuid, for each find minimal timestamp and set it as aggregate row's timestamp. We can groupby and get first of each (we make the assumption that the data is in chronological order).."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first_from_each_requestuuid = merged_requests[merged_requests.groupby('request_uuid')['timestamp'].rank() == 1 ]\n",
    "first_from_each_requestuuid = merged_requests.drop_duplicates(\"request_uuid\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130915"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_from_each_requestuuid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many removed?\n",
    "\n",
    "Before: 2997586 After: 130915\n",
    "Decrease of 95.63265240763735%\n",
    "\n",
    "We see that there are A LOT of duplicates based already only on request_uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 2997586 After: 130915\n",
      "Decrease of 95.63265240763735%\n"
     ]
    }
   ],
   "source": [
    "before_count = len(merged_requests)\n",
    "after_count =  len(first_from_each_requestuuid)\n",
    "\n",
    "print(\"Before:\", before_count, \"After:\", after_count)\n",
    "print(\"Decrease of {}%\".format((100-(after_count/before_count)*100)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Saving data frame (without duplicates by requestuuid)\n",
    "\n",
    "first_from_each_requestuuid.to_csv(\"rate_quote_1_dup2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2 - Removing extra duplicates - different request_uuid, but other parameters same (nearby timestamp) \n",
    "\n",
    "1) Group by broker name\n",
    "\n",
    "2) Order each group by time\n",
    "\n",
    "3) Check for orders within 0.5s if other parameters but contract_id / request_uuid are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Group by broker_name\n",
    "\n",
    "grouped_by_broker = first_from_each_requestuuid.groupby(first_from_each_requestuuid[\"Broker_name\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 51194 duplicates\n"
     ]
    }
   ],
   "source": [
    "MAX_TIME_DIFFERENCE = 200000\n",
    "\n",
    "filtered_by_broker = {}\n",
    "# For every order find all orders in time difference of 1 second.\n",
    "duplicate_count = 0\n",
    "for group in grouped_by_broker:\n",
    "    #ordered_df = group[1].sort_values(by=\"timestamp\")\n",
    "    previous_row = None\n",
    "    in_difference = []\n",
    "    # Group by all other columns now..\n",
    "    all_other = group[1].groupby([\"pickup_timestamp\", \"return_timestamp\", \"driver_age\", \n",
    "          \"source_country_region_id\", \"pickup_desk_id\", \"return_desk_id\", \"rental_days\"])\n",
    "    filtered_rows = []\n",
    "    for group_other in all_other:\n",
    "        SAME_PARAMETERS = group_other[1].sort_values(by=\"timestamp\")\n",
    "        # Now from inside group find merge which ones have similar timestamp (Need to set an E - 0.5s?) Consult on this \n",
    "        previous_row = None\n",
    "        for row in SAME_PARAMETERS.itertuples():\n",
    "            if previous_row:\n",
    "                # If time difference is enough then add the row.\n",
    "                if (row[1] - previous_row[1]).microseconds > MAX_TIME_DIFFERENCE:\n",
    "                    filtered_rows.append(row)\n",
    "                else:\n",
    "                    duplicate_count += 1\n",
    "            else:\n",
    "                # We add first one no matter what.\n",
    "                filtered_rows.append(row)\n",
    "            previous_row = row\n",
    "    filtered_by_broker[group[0]] = filtered_rows\n",
    "\n",
    "    \n",
    "print(\"Removed {} duplicates\".format(duplicate_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing duplicates\n",
      "BSP Auto 1577\n",
      "Flexible Autos 150\n",
      "Online Republic 30\n",
      "RateChain test 55\n",
      "Skyscanner 8\n",
      "SupplierWebsite 904\n",
      "TravelJigsaw 128177\n",
      "Zuzuche 14\n",
      "\n",
      "After removing duplicates\n",
      "BSP Auto 1484\n",
      "Flexible Autos 132\n",
      "Online Republic 20\n",
      "RateChain test 55\n",
      "Skyscanner 8\n",
      "SupplierWebsite 868\n",
      "TravelJigsaw 77144\n",
      "Zuzuche 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Before removing duplicates\")\n",
    "\n",
    "for group in grouped_by_broker:\n",
    "    print(group[0], len(group[1]))\n",
    "\n",
    "print(\"\\nAfter removing duplicates\")\n",
    "    \n",
    "for k,v in filtered_by_broker.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79721\n"
     ]
    }
   ],
   "source": [
    "filtered_df = pd.DataFrame([item for sub in filtered_by_broker.keys() for item in filtered_by_broker[sub]])\n",
    "print(len(filtered_df))\n",
    "\n",
    "\n",
    "filtered_df.to_csv(\"new_rate_quote_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 2997586 After: 79721\n",
      "Decrease of 97.34049331695572%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Before:\", before_count, \"After:\", len(filtered_df))\n",
    "print(\"Decrease of {}%\".format((100-(len(filtered_df)/before_count)*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
